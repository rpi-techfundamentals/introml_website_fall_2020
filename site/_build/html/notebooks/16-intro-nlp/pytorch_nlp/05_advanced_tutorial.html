

<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Advanced: Making Dynamic Decisions and the Bi-LSTM CRF &#8212; MGMT 4100/6560 Introduction to Machine Learning Applications @Rensselaer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous">
    <link href="../../../_static/css/index.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/jupyter-sphinx.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/togglebutton.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/mystnb.js"></script>
    <script src="../../../_static/sphinx-book-theme.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="docsearch:language" content="en">



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
  
  <img src="../../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">MGMT 4100/6560 Introduction to Machine Learning Applications @Rensselaer</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../intro.html">
   Welcome to Introduction to Machine Learning Applications
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  OVERVIEW
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../content/schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../content/syllabus.html">
   Syllabus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../content/preparation.html">
   Before Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../content/in_class.html">
   In Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../content/assignments.html">
   Assignments
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  SESSIONS
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../sessions/session1.html">
   1. Course Overview &amp; Introduction to the Data Science Lifecycle (08/31)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../sessions/session2.html">
   2. Python Basics (09/03)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../sessions/session3.html">
   3. Python Basics  (First in Person Class, Tuesday follow Monday Schedule) (09/08)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../sessions/session4.html">
   4. Python conditionals, loops, functions, aggregating.  (09/10)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  NOTEBOOKS
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../01-intro-python/01-python-overview.html">
   1. Overview of Python Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01-intro-python/02-datastructures.html">
   2. Introduction Datastructures (Varibles, Lists, Dictionaries, and Sets)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01-intro-python/03-numpy.html">
   3. Overview of Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01-intro-python/04-pandas.html">
   4. Large sections of this were adopted from Analyzing structured data with Pandas by Steve Phelps.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../01-intro-python/04-pandas.html#introduction-to-pandas">
   5. Introduction to Pandas
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/notebooks/16-intro-nlp/pytorch_nlp/05_advanced_tutorial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/rpi-techfundamentals/introml_website_fall_2020"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/rpi-techfundamentals/introml_website_fall_2020/issues/new?title=Issue%20on%20page%20%2Fnotebooks/16-intro-nlp/pytorch_nlp/05_advanced_tutorial.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/rpi-techfundamentals/introml_website_fall_2020/blob/master/site/notebooks/16-intro-nlp/pytorch_nlp/05_advanced_tutorial.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-versus-static-deep-learning-toolkits">
   Dynamic versus Static Deep Learning Toolkits
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bi-lstm-conditional-random-field-discussion">
   Bi-LSTM Conditional Random Field Discussion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-notes">
   Implementation Notes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercise-a-new-loss-function-for-discriminative-tagging">
   Exercise: A new loss function for discriminative tagging
  </a>
 </li>
</ul>

        </nav>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="advanced-making-dynamic-decisions-and-the-bi-lstm-crf">
<h1>Advanced: Making Dynamic Decisions and the Bi-LSTM CRF<a class="headerlink" href="#advanced-making-dynamic-decisions-and-the-bi-lstm-crf" title="Permalink to this headline">¶</a></h1>
<div class="section" id="dynamic-versus-static-deep-learning-toolkits">
<h2>Dynamic versus Static Deep Learning Toolkits<a class="headerlink" href="#dynamic-versus-static-deep-learning-toolkits" title="Permalink to this headline">¶</a></h2>
<p>Pytorch is a <em>dynamic</em> neural network kit. Another example of a dynamic
kit is <code class="docutils literal notranslate"><span class="pre">Dynet</span> <span class="pre">&lt;https://github.com/clab/dynet&gt;</span></code>__ (I mention this because
working with Pytorch and Dynet is similar. If you see an example in
Dynet, it will probably help you implement it in Pytorch). The opposite
is the <em>static</em> tool kit, which includes Theano, Keras, TensorFlow, etc.
The core difference is the following:</p>
<ul class="simple">
<li><p>In a static toolkit, you define
a computation graph once, compile it, and then stream instances to it.</p></li>
<li><p>In a dynamic toolkit, you define a computation graph <em>for each
instance</em>. It is never compiled and is executed on-the-fly</p></li>
</ul>
<p>Without a lot of experience, it is difficult to appreciate the
difference. One example is to suppose we want to build a deep
constituent parser. Suppose our model involves roughly the following
steps:</p>
<ul class="simple">
<li><p>We build the tree bottom up</p></li>
<li><p>Tag the root nodes (the words of the sentence)</p></li>
<li><p>From there, use a neural network and the embeddings
of the words to find combinations that form constituents. Whenever you
form a new constituent, use some sort of technique to get an embedding
of the constituent. In this case, our network architecture will depend
completely on the input sentence. In the sentence “The green cat
scratched the wall”, at some point in the model, we will want to combine
the span <span class="math notranslate nohighlight">\((i,j,r) = (1, 3, \text{NP})\)</span> (that is, an NP constituent
spans word 1 to word 3, in this case “The green cat”).</p></li>
</ul>
<p>However, another sentence might be “Somewhere, the big fat cat scratched
the wall”. In this sentence, we will want to form the constituent
<span class="math notranslate nohighlight">\((2, 4, NP)\)</span> at some point. The constituents we will want to form
will depend on the instance. If we just compile the computation graph
once, as in a static toolkit, it will be exceptionally difficult or
impossible to program this logic. In a dynamic toolkit though, there
isn’t just 1 pre-defined computation graph. There can be a new
computation graph for each instance, so this problem goes away.</p>
<p>Dynamic toolkits also have the advantage of being easier to debug and
the code more closely resembling the host language (by that I mean that
Pytorch and Dynet look more like actual Python code than Keras or
Theano).</p>
</div>
<div class="section" id="bi-lstm-conditional-random-field-discussion">
<h2>Bi-LSTM Conditional Random Field Discussion<a class="headerlink" href="#bi-lstm-conditional-random-field-discussion" title="Permalink to this headline">¶</a></h2>
<p>For this section, we will see a full, complicated example of a Bi-LSTM
Conditional Random Field for named-entity recognition. The LSTM tagger
above is typically sufficient for part-of-speech tagging, but a sequence
model like the CRF is really essential for strong performance on NER.
Familiarity with CRF’s is assumed. Although this name sounds scary, all
the model is is a CRF but where an LSTM provides the features. This is
an advanced model though, far more complicated than any earlier model in
this tutorial. If you want to skip it, that is fine. To see if you’re
ready, see if you can:</p>
<ul class="simple">
<li><p>Write the recurrence for the viterbi variable at step i for tag k.</p></li>
<li><p>Modify the above recurrence to compute the forward variables instead.</p></li>
<li><p>Modify again the above recurrence to compute the forward variables in
log-space (hint: log-sum-exp)</p></li>
</ul>
<p>If you can do those three things, you should be able to understand the
code below. Recall that the CRF computes a conditional probability. Let
<span class="math notranslate nohighlight">\(y\)</span> be a tag sequence and <span class="math notranslate nohighlight">\(x\)</span> an input sequence of words.
Then we compute</p>
<p>\begin{align}P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y’} \exp{(\text{Score}(x, y’)})}\end{align}</p>
<p>Where the score is determined by defining some log potentials
<span class="math notranslate nohighlight">\(\log \psi_i(x,y)\)</span> such that</p>
<p>\begin{align}\text{Score}(x,y) = \sum_i \log \psi_i(x,y)\end{align}</p>
<p>To make the partition function tractable, the potentials must look only
at local features.</p>
<p>In the Bi-LSTM CRF, we define two kinds of potentials: emission and
transition. The emission potential for the word at index <span class="math notranslate nohighlight">\(i\)</span> comes
from the hidden state of the Bi-LSTM at timestep <span class="math notranslate nohighlight">\(i\)</span>. The
transition scores are stored in a <span class="math notranslate nohighlight">\(|T|x|T|\)</span> matrix
<span class="math notranslate nohighlight">\(\textbf{P}\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is the tag set. In my
implementation, <span class="math notranslate nohighlight">\(\textbf{P}_{j,k}\)</span> is the score of transitioning
to tag <span class="math notranslate nohighlight">\(j\)</span> from tag <span class="math notranslate nohighlight">\(k\)</span>. So:</p>
<p>\begin{align}\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i)\end{align}</p>
<p>\begin{align}= \sum_i h_i[y_i] + \textbf{P}<em>{y_i, y</em>{i-1}}\end{align}</p>
<p>where in this second expression, we think of the tags as being assigned
unique non-negative indices.</p>
<p>If the above discussion was too brief, you can check out
<code class="docutils literal notranslate"><span class="pre">this</span> <span class="pre">&lt;http://www.cs.columbia.edu/%7Emcollins/crf.pdf&gt;</span></code>__ write up from
Michael Collins on CRFs.</p>
</div>
<div class="section" id="implementation-notes">
<h2>Implementation Notes<a class="headerlink" href="#implementation-notes" title="Permalink to this headline">¶</a></h2>
<p>The example below implements the forward algorithm in log space to
compute the partition function, and the viterbi algorithm to decode.
Backpropagation will compute the gradients automatically for us. We
don’t have to do anything by hand.</p>
<p>The implementation is not optimized. If you understand what is going on,
you’ll probably quickly see that iterating over the next tag in the
forward algorithm could probably be done in one big operation. I wanted
to code to be more readable. If you want to make the relevant change,
you could probably use this tagger for real tasks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Robert Guthrie</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span> <span class="k">as</span> <span class="nn">autograd</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Helper functions to make the code more readable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="c1"># return the argmax as a python int</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">idx</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">prepare_sequence</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">to_ix</span><span class="p">):</span>
    <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_ix</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>


<span class="c1"># Compute log sum exp in a numerically stable way for the forward algorithm</span>
<span class="k">def</span> <span class="nf">log_sum_exp</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">max_score</span> <span class="o">=</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">argmax</span><span class="p">(</span><span class="n">vec</span><span class="p">)]</span>
    <span class="n">max_score_broadcast</span> <span class="o">=</span> <span class="n">max_score</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vec</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">max_score</span> <span class="o">+</span> \
        <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">vec</span> <span class="o">-</span> <span class="n">max_score_broadcast</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p>Create model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BiLSTM_CRF</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">tag_to_ix</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BiLSTM_CRF</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span> <span class="o">=</span> <span class="n">tag_to_ix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tag_to_ix</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">word_embeds</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                            <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Maps the output of the LSTM into tag space.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2tag</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">)</span>

        <span class="c1"># Matrix of transition parameters.  Entry i,j is the score of</span>
        <span class="c1"># transitioning *to* i *from* j.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">))</span>

        <span class="c1"># These two statements enforce the constraint that we never transfer</span>
        <span class="c1"># to the start tag and we never transfer from the stop tag</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">START_TAG</span><span class="p">],</span> <span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="n">tag_to_ix</span><span class="p">[</span><span class="n">STOP_TAG</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10000</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_forward_alg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feats</span><span class="p">):</span>
        <span class="c1"># Do the forward algorithm to compute the partition function</span>
        <span class="n">init_alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.</span><span class="p">)</span>
        <span class="c1"># START_TAG has all of the score.</span>
        <span class="n">init_alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">START_TAG</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.</span>

        <span class="c1"># Wrap in a variable so that we will get automatic backprop</span>
        <span class="n">forward_var</span> <span class="o">=</span> <span class="n">init_alphas</span>

        <span class="c1"># Iterate through the sentence</span>
        <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">feats</span><span class="p">:</span>
            <span class="n">alphas_t</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># The forward tensors at this timestep</span>
            <span class="k">for</span> <span class="n">next_tag</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">):</span>
                <span class="c1"># broadcast the emission score: it is the same regardless of</span>
                <span class="c1"># the previous tag</span>
                <span class="n">emit_score</span> <span class="o">=</span> <span class="n">feat</span><span class="p">[</span><span class="n">next_tag</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">)</span>
                <span class="c1"># the ith entry of trans_score is the score of transitioning to</span>
                <span class="c1"># next_tag from i</span>
                <span class="n">trans_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="p">[</span><span class="n">next_tag</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="c1"># The ith entry of next_tag_var is the value for the</span>
                <span class="c1"># edge (i -&gt; next_tag) before we do log-sum-exp</span>
                <span class="n">next_tag_var</span> <span class="o">=</span> <span class="n">forward_var</span> <span class="o">+</span> <span class="n">trans_score</span> <span class="o">+</span> <span class="n">emit_score</span>
                <span class="c1"># The forward variable for this tag is log-sum-exp of all the</span>
                <span class="c1"># scores.</span>
                <span class="n">alphas_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_sum_exp</span><span class="p">(</span><span class="n">next_tag_var</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">forward_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">alphas_t</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">terminal_var</span> <span class="o">=</span> <span class="n">forward_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">STOP_TAG</span><span class="p">]]</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">log_sum_exp</span><span class="p">(</span><span class="n">terminal_var</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">alpha</span>

    <span class="k">def</span> <span class="nf">_get_lstm_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">()</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeds</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">lstm_out</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">lstm_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2tag</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lstm_feats</span>

    <span class="k">def</span> <span class="nf">_score_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feats</span><span class="p">,</span> <span class="n">tags</span><span class="p">):</span>
        <span class="c1"># Gives the score of a provided tag sequence</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">START_TAG</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span> <span class="n">tags</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">feat</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">feats</span><span class="p">):</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="p">[</span><span class="n">tags</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tags</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+</span> <span class="n">feat</span><span class="p">[</span><span class="n">tags</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">STOP_TAG</span><span class="p">],</span> <span class="n">tags</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="k">return</span> <span class="n">score</span>

    <span class="k">def</span> <span class="nf">_viterbi_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feats</span><span class="p">):</span>
        <span class="n">backpointers</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Initialize the viterbi variables in log space</span>
        <span class="n">init_vvars</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.</span><span class="p">)</span>
        <span class="n">init_vvars</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">START_TAG</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># forward_var at step i holds the viterbi variables for step i-1</span>
        <span class="n">forward_var</span> <span class="o">=</span> <span class="n">init_vvars</span>
        <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">feats</span><span class="p">:</span>
            <span class="n">bptrs_t</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># holds the backpointers for this step</span>
            <span class="n">viterbivars_t</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># holds the viterbi variables for this step</span>

            <span class="k">for</span> <span class="n">next_tag</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tagset_size</span><span class="p">):</span>
                <span class="c1"># next_tag_var[i] holds the viterbi variable for tag i at the</span>
                <span class="c1"># previous step, plus the score of transitioning</span>
                <span class="c1"># from tag i to next_tag.</span>
                <span class="c1"># We don&#39;t include the emission scores here because the max</span>
                <span class="c1"># does not depend on them (we add them in below)</span>
                <span class="n">next_tag_var</span> <span class="o">=</span> <span class="n">forward_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="p">[</span><span class="n">next_tag</span><span class="p">]</span>
                <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">next_tag_var</span><span class="p">)</span>
                <span class="n">bptrs_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_tag_id</span><span class="p">)</span>
                <span class="n">viterbivars_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_tag_var</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">best_tag_id</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># Now add in the emission scores, and assign forward_var to the set</span>
            <span class="c1"># of viterbi variables we just computed</span>
            <span class="n">forward_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">viterbivars_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">feat</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">backpointers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bptrs_t</span><span class="p">)</span>

        <span class="c1"># Transition to STOP_TAG</span>
        <span class="n">terminal_var</span> <span class="o">=</span> <span class="n">forward_var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">transitions</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">STOP_TAG</span><span class="p">]]</span>
        <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">terminal_var</span><span class="p">)</span>
        <span class="n">path_score</span> <span class="o">=</span> <span class="n">terminal_var</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">best_tag_id</span><span class="p">]</span>

        <span class="c1"># Follow the back pointers to decode the best path.</span>
        <span class="n">best_path</span> <span class="o">=</span> <span class="p">[</span><span class="n">best_tag_id</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">bptrs_t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">backpointers</span><span class="p">):</span>
            <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">bptrs_t</span><span class="p">[</span><span class="n">best_tag_id</span><span class="p">]</span>
            <span class="n">best_path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">best_tag_id</span><span class="p">)</span>
        <span class="c1"># Pop off the start tag (we dont want to return that to the caller)</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">best_path</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">start</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">START_TAG</span><span class="p">]</span>  <span class="c1"># Sanity check</span>
        <span class="n">best_path</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">path_score</span><span class="p">,</span> <span class="n">best_path</span>

    <span class="k">def</span> <span class="nf">neg_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tags</span><span class="p">):</span>
        <span class="n">feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_lstm_features</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
        <span class="n">forward_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_alg</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>
        <span class="n">gold_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_score_sentence</span><span class="p">(</span><span class="n">feats</span><span class="p">,</span> <span class="n">tags</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">forward_score</span> <span class="o">-</span> <span class="n">gold_score</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>  <span class="c1"># dont confuse this with _forward_alg above.</span>
        <span class="c1"># Get the emission scores from the BiLSTM</span>
        <span class="n">lstm_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_lstm_features</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

        <span class="c1"># Find the best path, given the features.</span>
        <span class="n">score</span><span class="p">,</span> <span class="n">tag_seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_viterbi_decode</span><span class="p">(</span><span class="n">lstm_feats</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_seq</span>
</pre></div>
</div>
</div>
</div>
<p>Run training</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">START_TAG</span> <span class="o">=</span> <span class="s2">&quot;&lt;START&gt;&quot;</span>
<span class="n">STOP_TAG</span> <span class="o">=</span> <span class="s2">&quot;&lt;STOP&gt;&quot;</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Make up some training data</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="p">[(</span>
    <span class="s2">&quot;the wall street journal reported today that apple corporation made money&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span>
    <span class="s2">&quot;B I I I O O O B I O O&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="p">),</span> <span class="p">(</span>
    <span class="s2">&quot;georgia tech is a university in georgia&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span>
    <span class="s2">&quot;B I O O O O B&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="p">)]</span>

<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_to_ix</span><span class="p">:</span>
            <span class="n">word_to_ix</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>

<span class="n">tag_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;B&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;I&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;O&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">START_TAG</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="n">STOP_TAG</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BiLSTM_CRF</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">),</span> <span class="n">tag_to_ix</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="c1"># Check predictions before training</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">precheck_sent</span> <span class="o">=</span> <span class="n">prepare_sequence</span><span class="p">(</span><span class="n">training_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">word_to_ix</span><span class="p">)</span>
    <span class="n">precheck_tags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">precheck_sent</span><span class="p">))</span>

<span class="c1"># Make sure prepare_sequence from earlier in the LSTM section is loaded</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span>
        <span class="mi">300</span><span class="p">):</span>  <span class="c1"># again, normally you would NOT do 300 epochs, it is toy data</span>
    <span class="k">for</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tags</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
        <span class="c1"># Step 1. Remember that Pytorch accumulates gradients.</span>
        <span class="c1"># We need to clear them out before each instance</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Step 2. Get our inputs ready for the network, that is,</span>
        <span class="c1"># turn them into Tensors of word indices.</span>
        <span class="n">sentence_in</span> <span class="o">=</span> <span class="n">prepare_sequence</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tag_to_ix</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

        <span class="c1"># Step 3. Run our forward pass.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">neg_log_likelihood</span><span class="p">(</span><span class="n">sentence_in</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="c1"># Step 4. Compute the loss, gradients, and update the parameters by</span>
        <span class="c1"># calling optimizer.step()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># Check predictions after training</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">precheck_sent</span> <span class="o">=</span> <span class="n">prepare_sequence</span><span class="p">(</span><span class="n">training_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">word_to_ix</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">precheck_sent</span><span class="p">))</span>
<span class="c1"># We got it!</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-a-new-loss-function-for-discriminative-tagging">
<h2>Exercise: A new loss function for discriminative tagging<a class="headerlink" href="#exercise-a-new-loss-function-for-discriminative-tagging" title="Permalink to this headline">¶</a></h2>
<p>It wasn’t really necessary for us to create a computation graph when
doing decoding, since we do not backpropagate from the viterbi path
score. Since we have it anyway, try training the tagger where the loss
function is the difference between the Viterbi path score and the score
of the gold-standard path. It should be clear that this function is
non-negative and 0 when the predicted tag sequence is the correct tag
sequence. This is essentially <em>structured perceptron</em>.</p>
<p>This modification should be short, since Viterbi and score_sentence are
already implemented. This is an example of the shape of the computation
graph <em>depending on the training instance</em>. Although I haven’t tried
implementing this in a static toolkit, I imagine that it is possible but
much less straightforward.</p>
<p>Pick up some real data and do a comparison!</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks/16-intro-nlp/pytorch_nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Jason Kuruzovich<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    <script src="../../../_static/js/index.js"></script>
    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-32817743-6', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>