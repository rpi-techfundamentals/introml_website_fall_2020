{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"nteract":{"version":"0.12.3"},"colab":{"name":"Copy of 02-intro-spark.ipynb","provenance":[{"file_id":"https://github.com/rpi-techfundamentals/introml_website_fall_2020/blob/master/site/notebooks/10-big-data/02-intro-spark.ipynb","timestamp":1607027104903}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"EDpkz0GlRCpL"},"source":["[![AnalyticsDojo](https://github.com/rpi-techfundamentals/spring2019-materials/blob/master/fig/final-logo.png?raw=1)](http://rpi.analyticsdojo.com)\n","<center><h1>Introduction to Spark</h1></center>\n","<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>"]},{"cell_type":"markdown","metadata":{"id":"RvDUNVYbRCpL"},"source":["# Introduction to Spark\n","Adopted from work by Steve Phelps:\n","https://github.com/phelps-sg/python-bigdata \n","This work is licensed under the Creative Commons Attribution 4.0 International license agreement.\n"]},{"cell_type":"markdown","metadata":{"id":"UXW_YfsuRCpL"},"source":["### Reference\n","- [Spark Documentation](http://spark.apache.org/docs/latest/)\n","- [Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)\n","- [DataBricks Login](https://community.cloud.databricks.com)\n","- [Pyspark](https://github.com/jupyter/docker-stacks)\n","To install pyspark\n","\n","``` \n","   !pip install pyspark\n","```"]},{"cell_type":"code","metadata":{"id":"RdF5pwrdRIPU","executionInfo":{"status":"ok","timestamp":1607026957537,"user_tz":300,"elapsed":37803,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"20bbcd8b-4e89-46ac-d015-a6b4acc5c565","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pyspark"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 67kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 38.3MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=1a9e4bc82e720a3f1f694e7fb88a7ef0cf8c63bf2e37e709f1f8768f1695772a\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2HomtYO-RCpL"},"source":["\n","### Overview\n","- History\n","- Data Structures\n","- Using Apache Spark with Python\n"]},{"cell_type":"markdown","metadata":{"id":"qICPQY4RRCpL"},"source":["## History\n","\n","- Apache Spark was first released in 2014. \n","\n","- It was originally developed by [Matei Zaharia](http://people.csail.mit.edu/matei) as a class project, and later a PhD dissertation, at University of California, Berkeley.\n","\n","- In contrast to Hadoop, Apache Spark:\n","\n","    - is easy to install and configure.\n","    - provides a much more natural *iterative* workflow \n"]},{"cell_type":"markdown","metadata":{"id":"FrVmm-gIRCpL"},"source":["## Resilient Distributed Datasets (RDD)\n","\n","- The fundamental abstraction of Apache Spark is a read-only, parallel, distributed, fault-tolerent collection called a resilient distributed datasets (RDD).\n","\n","- When working with Apache Spark we iteratively apply functions to every elelement of these collections in parallel to produce *new* RDDs.\n","\n","- For the most part, you can think/use RDDs like distributed dataframes. \n"]},{"cell_type":"markdown","metadata":{"id":"eDpPSboSRCpL"},"source":["## Resilient Distributed Datasets (RDD)\n","\n","- Properties resilient distributed datasets (RDDs):\n","    - The data is distributed across nodes in a cluster of computers.\n","    - No data is lost if a single node fails.\n","    - Data is typically stored in HBase tables, or HDFS files.\n","    - The `map` and `reduce` functions can work in *parallel* across\n","       different keys, or different elements of the collection.\n","\n","- The underlying framework (e.g. Hadoop or Apache Spark) allocates data and processing to different nodes, without any intervention from the programmer."]},{"cell_type":"markdown","metadata":{"id":"bso2HU5aRCpL"},"source":["## Word Count Example\n","\n","- In this simple example, the input is a set of URLs, each record is a document. <br> <br> <br>\n","\n","- **Problem: Compute how many times each word has occurred across data set.**"]},{"cell_type":"markdown","metadata":{"id":"QxXj1N-_RCpL"},"source":["## Word Count: Map \n","\n","\n","The input to $\\operatorname{map}$ is a mapping:\n","- Key: URL\n","- Value: Contents of document <br>\n","$\\left< document1, to \\; be \\; or \\; not \\; to \\; be \\right>$  \n","    \n","\n","- In this example, our $\\operatorname{map}$ function will process a given URL, and produces a mapping:\n","- So our original data-set will be transformed to:\n","  \n","  $\\left< to, 1 \\right>$\n","  $\\left< be, 1 \\right>$\n","  $\\left< or, 1 \\right>$\n","  $\\left< not, 1 \\right>$\n","  $\\left< to, 1 \\right>$\n","  $\\left< be, 1 \\right>$"]},{"cell_type":"markdown","metadata":{"id":"GdfvZyLPRCpL"},"source":["## Word Count: Reduce\n","\n","\n","- The reduce operation groups values according to their key, and then performs areduce on each key.\n","\n","- The collections are partitioned across different storage units, therefore.\n","\n","- Map-Reduce will fold the data in such a way that it minimises data-copying across the cluster.\n","\n","- Data in different partitions are reduced separately in parallel.\n","\n","- The final result is a reduce of the reduced data in each partition.\n","\n","- Therefore it is very important that our operator *is both commutative and associative*.\n","\n","- In our case the function is the `+` operator\n","\n","  $\\left< be, 2 \\right>$  \n","  $\\left< not, 1 \\right>$  \n","  $\\left< or, 1 \\right>$  \n","  $\\left< to, 2 \\right>$  \n","  "]},{"cell_type":"markdown","metadata":{"id":"pHF5txhqRCpL"},"source":["## Map-Reduce on a Cluster of Computers\n","\n","- The code we have written so far will *not* allow us to exploit parallelism from multiple computers in a [cluster](https://en.wikipedia.org/wiki/Computer_cluster).\n","\n","- Developing such a framework would be a very large software engineering project.\n","\n","- There are existing frameworks we can use:\n","    - [Apache Hadoop](https://hadoop.apache.org/)\n","    - [Apache Spark](https://spark.apache.org/)\n","    \n","- This notebook covers Apache Spark."]},{"cell_type":"markdown","metadata":{"id":"JLXN1NN6RCpL"},"source":["## Apache Spark\n","\n","- Apache Spark provides an object-oriented library for processing data on the cluster.\n","\n","- It provides objects which represent resilient distributed datasets (RDDs).\n","\n","- RDDs behave a bit like Python collections (e.g. lists).\n","\n","- However:\n","    - the underlying data is distributed across the nodes in the cluster, and\n","    - the collections are *immutable*."]},{"cell_type":"markdown","metadata":{"id":"mYkLJa_jRCpL"},"source":["## Apache Spark and Map-Reduce\n","\n","- We process the data by using higher-order functions to map RDDs onto *new* RDDs. \n","\n","- Each instance of an RDD has at least two *methods* corresponding to the Map-Reduce workflow:\n","    - `map`\n","    - `reduceByKey`\n","    \n","- These methods work in the same way as the corresponding functions we defined earlier to work with the standard Python collections.  \n","\n","- There are also additional RDD methods in the Apache Spark API including ones for SQL.\n","   "]},{"cell_type":"markdown","metadata":{"id":"-m5AJ5ETRCpL"},"source":["## Word-count in Apache Spark\n","\n"]},{"cell_type":"code","metadata":{"id":"Y8rHLG8aRCpL","executionInfo":{"status":"ok","timestamp":1607026982912,"user_tz":300,"elapsed":351,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"53326e98-9d49-42d3-95a4-06462875239d","colab":{"base_uri":"https://localhost:8080/"}},"source":["words = \"to be or not to be\".split()\n","words"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['to', 'be', 'or', 'not', 'to', 'be']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Qfy0x3Y7RCpP"},"source":["### The `SparkContext` class\n","\n","- When working with Apache Spark we invoke methods on an object which is an instance of the `pyspark.context.SparkContext` context.\n","\n","- Typically, (such as when running on DataBricks) an instance of this object will be created automatically for you and assigned to the variable `sc`.\n","\n","- The `parallelize` method in `SparkContext` can be used to turn any ordinary Python collection into an RDD; "]},{"cell_type":"code","metadata":{"id":"khysYuCMRCpP","executionInfo":{"status":"ok","timestamp":1607027001208,"user_tz":300,"elapsed":6422,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["#Don't Execute this on Databricks\n","#To be used if executing via docker\n","import pyspark\n","sc = pyspark.SparkContext('local[*]')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAM8BEEtRCpP","executionInfo":{"status":"ok","timestamp":1607027008594,"user_tz":300,"elapsed":336,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"0b9be4fe-306c-45cc-e077-a76b1a8d1cde","colab":{"base_uri":"https://localhost:8080/"}},"source":["words_rdd = sc.parallelize(words)\n","words_rdd"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:262"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"SJiwXwu1RCpP"},"source":["### Mapping an RDD\n","\n","- Now when we invoke the `map` or `reduceByKey` methods on `my_rdd` we can set up a parallel processing computation across the cluster."]},{"cell_type":"code","metadata":{"id":"Djk9lXXnRCpP","executionInfo":{"status":"ok","timestamp":1607027008818,"user_tz":300,"elapsed":554,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"8ca4a870-de26-407f-c319-46dc4116f29b","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_tuples_rdd = words_rdd.map(lambda x: (x, 1))\n","word_tuples_rdd"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[2] at RDD at PythonRDD.scala:53"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"gX9bZgvTRCpP"},"source":["### Collecting the RDD\n","- Notice that we do not have a result yet.\n","\n","- The computation is not performed until we request the final result to be *collected*.\n","\n","- We do this by invoking the `collect()` method.\n","\n","- Be careful with the `collect` method, as all data you are collecting must fit in memory.  \n","\n","- The `take` method is similar to `collect`, but only returns the first $n$ elements.\n"," "]},{"cell_type":"code","metadata":{"id":"43wnW8mKRCpP","executionInfo":{"status":"ok","timestamp":1607027010367,"user_tz":300,"elapsed":2099,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"39ae4aab-3898-4661-886c-2d1d72dbddba","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_tuples_rdd.collect()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('to', 1), ('be', 1), ('or', 1), ('not', 1), ('to', 1), ('be', 1)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"1oNwaBVgRCpP","executionInfo":{"status":"ok","timestamp":1607027010750,"user_tz":300,"elapsed":2477,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"acde1875-79e8-4343-dc79-51d00c0ff2ff","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_tuples_rdd.take(4)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('to', 1), ('be', 1), ('or', 1), ('not', 1)]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"NnBPbTIgRCpP"},"source":["### Reducing an RDD\n","\n","- However, we require additional processing to reduce the data using the word key. "]},{"cell_type":"code","metadata":{"id":"Vf9SnfzMRCpP","executionInfo":{"status":"ok","timestamp":1607027010751,"user_tz":300,"elapsed":2474,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"1d6aa030-f811-4146-84b3-538e8f2e3ffc","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_counts_rdd = word_tuples_rdd.reduceByKey(lambda x, y: x + y)\n","word_counts_rdd"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonRDD[9] at RDD at PythonRDD.scala:53"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"F11Je6MORCpP"},"source":["- Now we request the final result:"]},{"cell_type":"code","metadata":{"id":"w6SpERTpRCpP","executionInfo":{"status":"ok","timestamp":1607027011901,"user_tz":300,"elapsed":3620,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"fae3ae6e-663d-4b30-9ed0-2b31f15a1198","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_counts = word_counts_rdd.collect()\n","word_counts"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"63pGYWbdRCpQ"},"source":["### Lazy evaluation \n","\n","- It is only when we invoke `collect()` that the processing is performed on the cluster.\n","\n","- Invoking `collect()` will cause both the `map` and `reduceByKey` operations to be performed.\n","\n","- If the resulting collection is very large then this can be an expensive operation.\n"]},{"cell_type":"code","metadata":{"id":"K-Q8v9EXRCpQ","executionInfo":{"status":"ok","timestamp":1607027012067,"user_tz":300,"elapsed":3782,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"f010988a-823e-4ee0-8ff3-c04bf4d36aba","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_counts_rdd.take(2)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('to', 2), ('be', 2)]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"T6oHciE-RCpQ"},"source":["### Connecting MapReduce in Single Command\n","- Can string together `map` and `reduce` commands.\n","- Not executed until it is collected."]},{"cell_type":"code","metadata":{"id":"XLCZsMwERCpQ","executionInfo":{"status":"ok","timestamp":1607027012411,"user_tz":300,"elapsed":4120,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"348a5bb6-226c-42f3-c10d-bb856d3b46be","colab":{"base_uri":"https://localhost:8080/"}},"source":["text = \"to be or not to be\".split()\n","rdd = sc.parallelize(text)\n","counts = rdd.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n","counts.collect()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"7kw0M1P2RCpQ"},"source":["## Additional RDD transformations\n","\n","- Apache Spark offers many more methods for operating on collections of tuples over and above the standard Map-Reduce framework:\n","\n","    - Sorting: `sortByKey`, `sortBy`, `takeOrdered`\n","    - Mapping: `flatMap`\n","    - Filtering: `filter`\n","    - Counting: `count`\n","    - Set-theoretic: `intersection`, `union`\n","    - Many others: [see the Transformations section of the programming guide](https://spark.apache.org/docs/latest/programming-guide.html#transformations)\n","    "]},{"cell_type":"markdown","metadata":{"id":"INvfPnBtRCpQ"},"source":["## Creating an RDD from a text file\n","\n","- In the previous example, we created an RDD from a Python collection.\n","\n","- This is *not* typically how we would work with big data.\n","\n","- More commonly we would create an RDD corresponding to data in an\n","HBase table, or an HDFS file.\n","\n","- The following example creates an RDD from a text file on the native filesystem (ext4);\n","    - With bigger data, you would use an HDFS file, but the principle is the same.\n","\n","- Each element of the RDD corresponds to a single *line* of text."]},{"cell_type":"code","metadata":{"id":"CmJ8ysFjRCpQ","executionInfo":{"status":"ok","timestamp":1607027012952,"user_tz":300,"elapsed":4658,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["genome = sc.textFile('../input/iris.csv')"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z4EMdAynRCpQ"},"source":["## Calculating $\\pi$ using Spark\n","\n","- We can estimate an approximate value for $\\pi$ using the following Monte-Carlo method:\n","\n","\n","1.    Inscribe a circle in a square\n","2.    Randomly generate points in the square\n","3.    Determine the number of points in the square that are also in the circle\n","4.    Let $r$ be the number of points in the circle divided by the number of points in the square, then $\\pi \\approx 4 r$.\n","    \n","- Note that the more points generated, the better the approximation\n","\n","See [this tutorial](https://computing.llnl.gov/tutorials/parallel_comp/#ExamplesPI)."]},{"cell_type":"code","metadata":{"id":"FZ64dWB8RCpQ","executionInfo":{"status":"ok","timestamp":1607027014823,"user_tz":300,"elapsed":6525,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"0806cef3-c7f1-4ab3-cf59-8ff341f667b0","colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","\n","def sample(p):\n","    #here x,y are the x,y coordinate\n","    x, y = np.random.random(), np.random.random()\n","    #Because the circle is of \n","    return 1 if x*x + y*y < 1 else 0\n","\n","NUM_SAMPLES = 1000000\n","\n","count = sc.parallelize(range(0, NUM_SAMPLES)).map(sample) \\\n","             .reduce(lambda a, b: a + b)\n","#Area  = 4*PI*r\n","r = float(count) / float(NUM_SAMPLES)\n","r\n","print (\"Pi is approximately %f\" % (4.0 * r))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Pi is approximately 3.138136\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BxTUS3shRCpQ","executionInfo":{"status":"ok","timestamp":1607027014823,"user_tz":300,"elapsed":6522,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":[""],"execution_count":18,"outputs":[]}]}