{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DesFFXa6SmRI",
        "colab_type": "text"
      },
      "source": [
        "## Lecture-21: Introduction to Natural Language Processing\n",
        "\n",
        "###Sentiment Analysis\n",
        "\n",
        "#### What is sentiment analysis? \n",
        "\n",
        "Take a few movie reviews as examples (taken from Prof. Jurafsky's lecture notes): \n",
        "\n",
        "1. unbelievably disappointing \n",
        "2. Full of zany characters and richly applied satire, and some great plot twists\n",
        "3. This is the greatest screwball comedy ever filmed\n",
        "4. It was pathetic. The worst part about it was the boxing scenes. \n",
        "\n",
        "\n",
        "*Positive*: 2, 3\n",
        "*Negative*: 1, 4\n",
        "\n",
        "\n",
        "![alt text](https://www.dropbox.com/s/zmowjdhfodh9na5/danlecture.tif)\n",
        "Google shopping; Bing shopping; Twitter sentiment about airline customer service\n",
        "\n",
        "#### Sentiment analysis is the detection of **attitudes** “enduring, affectively colored beliefs, dispositions towards objects or persons”\n",
        "\n",
        "\n",
        "1. **Holder (source)** of attitude\n",
        "2. **Target (aspect)** of attitude\n",
        "3. **Type** of attitude\n",
        "    * From a set of types\n",
        "        - Like, love, hate, value, desire, etc.\n",
        "    * Or (more commonly) simple weighted polarity: \n",
        "        - positive, negative, neutral, together with strength\n",
        "4. **Text** containing the attitude\n",
        "    * Sentence or entire document\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbjbAneLvRES",
        "colab_type": "code",
        "outputId": "4eec097b-10e5-41d9-dae2-4ed72f3c035f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# We will use vader sentiment analysis here considering short text phrases\n",
        "!pip install vaderSentiment\n",
        "import vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.6/dist-packages (3.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuOf3S6lY5C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure_sentiment(textval):\n",
        "  sentObj = SentimentIntensityAnalyzer() \n",
        "  sentimentvals = sentObj.polarity_scores(textval)\n",
        "  print(sentimentvals)\n",
        "  if sentimentvals['compound']>=0.5: \n",
        "    return(\"Positive\")\n",
        "  elif sentimentvals['compound']<= -0.5: \n",
        "    return(\"Negative\")\n",
        "  else:\n",
        "    return(\"Neutral\")\n",
        "\n",
        "text1 = \"I love the beautiful weather today. It is absolutely pleasant.\"\n",
        "text2 = \"Unbelievably disappointing\"\n",
        "text3 = \"Full of zany characters and richly applied satire, and some great plot twists\"\n",
        "text4 = \"This is the greatest screwball comedy ever filmed\"\n",
        "text5 = \"This is the greatest screwball comedy ever filmed\"\n",
        "text6 = \"It was pathetic. The worst part about it was the boxing scenes.\"\n",
        "\n",
        "#print(measure_sentiment(text1))\n",
        "#print(measure_sentiment(text2))\n",
        "#print(measure_sentiment(text3))\n",
        "#print(measure_sentiment(text4))\n",
        "#print(measure_sentiment(text5))\n",
        "print(measure_sentiment(text6))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk3J787uSqPu",
        "colab_type": "text"
      },
      "source": [
        "### Topic Modeling -- Latent Dirichlet Allocation \n",
        "\n",
        "#### Topic model is a type of statistical model to discover the abstract latent topics present in a given set of documents. \n",
        "\n",
        "#### Topic modeling allows us to discover the latent semantic structures in a text corpus through learning probabilistic distributions over words present in the document.  \n",
        "\n",
        "#### It is a generative statistical model that allows different classes of observations to be explained by groups of unobserved data similar to clustering. \n",
        "**It assumes that documents are probability distributions over topics and topics are probability distributions over words.**  \n",
        "\n",
        "#### Latent Dirichlet Allocation (LDA) was proposed by Blei et al. in 2003 LDA assumes that the document is a mixture of topics where each topic is a mixture of words assigned to a topic where the topic distribution is assumed to have a dirichlet prior. \n",
        "\n",
        "#### We consider the Python package “gensim”to perform topic modeling over the online reviews in our notebook.\n",
        "\n",
        "![alt text](https://www.dropbox.com/s/wk87yos1jmjnm26/lda.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50PKQQEQSvoC",
        "colab_type": "code",
        "outputId": "112e736a-2f06-41be-d691-b1de3cff2e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "\n",
        "#Load the file first\n",
        "!wget https://www.dropbox.com/s/o8lxi6yrezmt5em/reviews.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-07 22:03:45--  https://www.dropbox.com/s/o8lxi6yrezmt5em/reviews.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/o8lxi6yrezmt5em/reviews.txt [following]\n",
            "--2019-11-07 22:03:45--  https://www.dropbox.com/s/raw/o8lxi6yrezmt5em/reviews.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca49837b6228b70be8d77e61bc1.dl.dropboxusercontent.com/cd/0/inline/Ar_TmvTj6aB8uANEqIJKbxZu2qjWL2AIrR9DGtrYalyog06i9GD2Hv6zuVGLnpHoj7Tp-SDZUq1NmgtzS1w9p-RfSoXlIdmrOad1piGku8eWddl-nWPXPcD6-6dTI-0tF4g/file# [following]\n",
            "--2019-11-07 22:03:46--  https://uca49837b6228b70be8d77e61bc1.dl.dropboxusercontent.com/cd/0/inline/Ar_TmvTj6aB8uANEqIJKbxZu2qjWL2AIrR9DGtrYalyog06i9GD2Hv6zuVGLnpHoj7Tp-SDZUq1NmgtzS1w9p-RfSoXlIdmrOad1piGku8eWddl-nWPXPcD6-6dTI-0tF4g/file\n",
            "Resolving uca49837b6228b70be8d77e61bc1.dl.dropboxusercontent.com (uca49837b6228b70be8d77e61bc1.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uca49837b6228b70be8d77e61bc1.dl.dropboxusercontent.com (uca49837b6228b70be8d77e61bc1.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3851 (3.8K) [text/plain]\n",
            "Saving to: ‘reviews.txt’\n",
            "\n",
            "reviews.txt         100%[===================>]   3.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-11-07 22:03:46 (388 MB/s) - ‘reviews.txt’ saved [3851/3851]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVZiis0Hakx7",
        "colab_type": "code",
        "outputId": "4d19a8be-0f54-4821-9be9-2080b8d72cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "#from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSVN4CdSatEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "f=open('reviews.txt')\n",
        "text = f.read()\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "sentences=sent_tokenize(text)\n",
        "\n",
        "data_words = list(sent_to_words(sentences))\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "dictionary = corpora.Dictionary(data_words_nostops)\n",
        "corpus = [dictionary.doc2bow(text) for text in data_words_nostops]\n",
        "\n",
        "NUM_TOPICS = 2\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "print(\"ldamodel is built\")\n",
        "#ldamodel.save('model5.gensim')\n",
        "topics = ldamodel.print_topics(num_words=6)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOI0XQuISw8m",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0xs3KDCSy70",
        "colab_type": "code",
        "outputId": "098cfd75-fd2b-4fdc-ba84-d211d26fc91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "model = gensim.models.Word2Vec(data_words_nostops, min_count=1)\n",
        "#print(model.most_similar(\"fish\", topn=10))\n",
        "\n",
        "print(model.most_similar(\"bar\", topn=10))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('actual', 0.24399515986442566), ('blue', 0.23981201648712158), ('burgers', 0.23935973644256592), ('fish', 0.22530747950077057), ('hole', 0.2182062566280365), ('bit', 0.20433926582336426), ('may', 0.20417353510856628), ('little', 0.1970674693584442), ('refills', 0.19307652115821838), ('sign', 0.19287416338920593)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL7zI_nkxn5",
        "colab_type": "text"
      },
      "source": [
        "### Bag of words model and TF-IDF computations\n",
        "\n",
        "##### tf-idf stands for Term frequency-inverse document frequency. The tf-idf weight is a weight often used in information retrieval and text mining. Variations of the tf-idf weighting scheme are often used by search engines in scoring and ranking a document’s relevance given a query. \n",
        "\n",
        "##### This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus (data-set).\n",
        "\n",
        "**STEP-1: Normalized Term Frequency (tf)** -- tf(t, d) = N(t, d) / ||D||\n",
        "wherein, ||D|| = Total number of term in the document\n",
        "\n",
        "tf(t, d) = term frequency for a term t in document d.\n",
        "\n",
        "N(t, d)  = number of times a term t occurs in document d\n",
        "\n",
        "**STEP-2: Inverse Document Frequency (idf)** -- \n",
        "idf(t) = N/ df(t) = N/N(t)\n",
        "\n",
        "idf(t) = log(N/ df(t))\n",
        "\n",
        "idf(pizza) = log(Total Number Of Documents / Number Of Documents with term pizza in it)\n",
        "\n",
        "**STEP-3: tf-idf Scoring** \n",
        "\n",
        "tf-idf(t, d) = tf(t, d)* idf(t, d)\n",
        "\n",
        "Example:\n",
        "\n",
        "Consider a document containing 100 words wherein the word kitty appears 3 times. The term frequency (i.e., tf) for kitty is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word kitty appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
        "\n",
        "\n",
        "Doc1: I love delicious pizza\n",
        "Doc2: Pizza is delicious\n",
        "Doc3: Kitties love me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_EFzMfajMAI",
        "colab_type": "text"
      },
      "source": [
        "## Class exercise\n",
        "\n",
        "Data files we use for this exercise are here: https://www.dropbox.com/s/cvafrg25ljde5gr/Lecture21_exercise_1.txt?dl=0\n",
        "\n",
        "https://www.dropbox.com/s/9lqnclea9bs9cdv/lecture21_exercise_2.txt?dl=0\n",
        "\n",
        "### 1. Read the 2nd file and preprocess it by removing non-alphanumeric characters \n",
        "\n",
        "### 2. Perform sentiment analysis, topic modeling and identify the most commonly co-occurring words with \"religion\", \"politics\", \"guns\"\n",
        "\n",
        "### 3. Build a bag of words model and compute TF-IDF the query \"pizza\". "
      ]
    }
  ]
}