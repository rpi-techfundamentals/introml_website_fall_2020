{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qLm7tWHtSgn"
   },
   "source": [
    "[![AnalyticsDojo](https://github.com/rpi-techfundamentals/spring2019-materials/blob/master/fig/final-logo.png?raw=1)](http://introml.analyticsdojo.com)<center>\n",
    "<h1>Revisiting IRIS with PyTorch</h1></center>\n",
    "<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting IRIS with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTIZ9QhHtSgs"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iz0-ym9QtShI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZySUCLOtShL"
   },
   "outputs": [],
   "source": [
    "#Let's get rid of some imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#Define the model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn_t2XgDtShS"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "#Iris is available from the sklearn package\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63ca_uuWtShe"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNP12NKmtShg"
   },
   "source": [
    "### Shuffled with Stratified Splitting\n",
    "\n",
    "- Especially for relatively small datasets, it's better to stratify the split.  \n",
    "- Stratification means that we maintain the original class proportion of the dataset in the test and training sets. \n",
    "- For example, after we randomly split the dataset as shown in the previous code example, we have the following class proportions in percent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6cBRKqUtShi"
   },
   "source": [
    "So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P2G9Ti29tShj"
   },
   "outputs": [],
   "source": [
    "#Import Module\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                    train_size=0.8,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123,\n",
    "                                                    stratify=y)\n",
    "\n",
    "print('All:', np.bincount(y) / float(len(y)) * 100.0)\n",
    "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
    "print('Validation:', np.bincount(val_y) / float(len(val_y)) * 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Y-a_vY0x61h"
   },
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAHmfATZtShr"
   },
   "outputs": [],
   "source": [
    "#This gives the number of columns, used below. \n",
    "train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNptEQEN-V4y"
   },
   "outputs": [],
   "source": [
    "#This gives the number of classes, used below. \n",
    "len(np.unique(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJPUnL0w9YZn"
   },
   "outputs": [],
   "source": [
    "#Define training hyperprameters.\n",
    "batch_size = 60\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "size_hidden= 100\n",
    "\n",
    "#Calculate some other hyperparameters based on data.  \n",
    "batch_no = len(train_X) // batch_size  #batches\n",
    "cols=train_X.shape[1] #Number of columns in input matrix\n",
    "classes= len(np.unique(train_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OC8DC-UutShz"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(\"Executing the model on :\",device)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,cols,size_hidden,classes):\n",
    "        super(Net, self).__init__()\n",
    "        #Note that 17 is the number of columns in the input matrix. \n",
    "        self.fc1 = nn.Linear(cols, size_hidden)\n",
    "        #variety of # possible for hidden layer size is arbitrary, but needs to be consistent across layers.  3 is the number of classes in the output (died/survived)\n",
    "        self.fc2 = nn.Linear(size_hidden, classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "net = Net(cols, size_hidden, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOSSOIcU_YXw"
   },
   "source": [
    "See this on [Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).\n",
    "\n",
    "[Cross Entropy Loss Function](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LfbvR2BsCoHV"
   },
   "outputs": [],
   "source": [
    "#Adam is a specific flavor of gradient decent which is typically better\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zd02kBpuvGLY"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    #Shuffle just mixes up the dataset between epocs\n",
    "    train_X, train_y = shuffle(train_X, train_y)\n",
    "    # Mini batch learning\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.FloatTensor(train_X[start:end]))\n",
    "        labels = Variable(torch.LongTensor(train_y[start:end]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyCeZouHvvO0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#This is a little bit tricky to get the resulting prediction.  \n",
    "def calculate_accuracy(x,y=[]):\n",
    "    \"\"\"\n",
    "    This function will return the accuracy if passed x and y or return predictions if just passed x. \n",
    "    \"\"\"\n",
    "    # Evaluate the model with the test set. \n",
    "    X = Variable(torch.FloatTensor(x))  \n",
    "    result = net(X) #This outputs the probability for each class.\n",
    "    _, labels = torch.max(result.data, 1)\n",
    "    if len(y) != 0:\n",
    "        num_right = np.sum(labels.data.numpy() == y)\n",
    "        print('Accuracy {:.2f}'.format(num_right / len(y)), \"for a total of \", len(y), \"records\")\n",
    "        return pd.DataFrame(data= {'actual': y, 'predicted': labels.data.numpy()})\n",
    "    else:\n",
    "        print(\"returning predictions\")\n",
    "        return labels.data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93CNVTglwCFr"
   },
   "outputs": [],
   "source": [
    "result1=calculate_accuracy(train_X,train_y)\n",
    "result2=calculate_accuracy(val_X,val_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkcDA3DAtSh8"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deCNeoLktSh-"
   },
   "source": [
    "### Comparison: Prediction using Simple Nearest Neighbor Classifier \n",
    "- By evaluating our classifier performance on data that has been seen during training, we could get false confidence in the predictive power of our model. \n",
    "- In the worst case, it may simply memorize the training samples but completely fails classifying new, similar samples -- we really don't want to put such a system into production!\n",
    "- Instead of using the same dataset for training and testing (this is called \"resubstitution evaluation\"), it is much much better to use a train/test split in order to estimate how well your trained model is doing on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S829OCottSh_"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#This creates a model object.\n",
    "classifier = KNeighborsClassifier()\n",
    "#This fits the model object to the data.\n",
    "classifier.fit(train_X, train_y)\n",
    "#This creates the prediction. \n",
    "pred_y = classifier.predict(val_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7HwZfB3tSiC"
   },
   "source": [
    "### Scoring \n",
    "- We can manually calculate the accuracy as we have done before. \n",
    "- `metrics.accuracy_score` is passed the target value and the predicted value.Model objects also built in scoring functions.\n",
    "- Can also us a `classifier.score` component built into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OojwVAtPtSiD"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#This calculates the accuracy.\n",
    "print(\"Classifier score: \", classifier.score(train_X, train_y) )\n",
    "print(\"Classifier score: \", classifier.score(val_X, val_y) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncFtNf0GtSik"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03-pytorch_iris.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
